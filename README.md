
# ğŸ½ï¸ **Chicago & Dallas Food Inspection Pipeline**

### *End-to-End ETL â€¢ Data Quality â€¢ Delta Live Tables â€¢ Power BI Dashboard*

This project implements a complete **data engineering pipeline** for analyzing food inspection data from **Chicago** and **Dallas**. The workflow covers **data ingestion, transformation, validation, modeling, and visualization**, following Medallion Architecture principles using **Bronze â†’ Silver â†’ Gold** layers.

The repo includes:

* âœ”ï¸ **Delta Live Tables (DLT) pipeline** for ETL & data quality
* âœ”ï¸ **ER/Dimensional data model** for downstream analytics
* âœ”ï¸ **Interactive Power BI dashboard** with insights
* âœ”ï¸ **Full end-to-end process**, ready for enterprise-scale data pipelines

---

## ğŸš€ **Project Overview**

Many U.S. cities publish food inspection datasets, but schemas differ widely.
This project **consolidates Chicago and Dallas inspection data** into a unified analytical model by:

1. **Profiling and cleansing raw delimited files**
2. **Applying validation rules and schema alignment**
3. **Building ETL pipelines using Delta Live Tables**
4. **Creating a dimensional model (ER Studio)**
5. **Delivering business insights through Power BI**

---

## ğŸ—ï¸ **Architecture**

### **Medallion Architecture:**

```
Bronze  â†’  Silver  â†’  Gold
Raw        Cleaned     Analytics-Ready Tables
```

### **Pipeline Components**

| Layer      | Description                                                                       |
| ---------- | --------------------------------------------------------------------------------- |
| **Bronze** | Raw ingestion of Chicago & Dallas CSV files using Databricks Auto Loader          |
| **Silver** | Cleansing, schema normalization, type casting, null handling, and DQ checks       |
| **Gold**   | Dimensional model (dim_facility, dim_address, fact_inspection) for BI consumption |

---

## ğŸ“ **Repository Structure**

```
Food_Inspection/
â”‚
â”œâ”€â”€ FoodInspectionDLT.ipynb            # Databricks Delta Live Tables ETL pipeline
â”œâ”€â”€ er model-FoodInspection-.DM1       # ER/Studio Dimensional Model
â”œâ”€â”€ Food_Inspection_Dashboard.pbix     # Power BI interactive dashboard
â””â”€â”€ README.md                           # Project documentation
```

---

## ğŸ”§ **Technologies Used**

* **Databricks** (Delta Live Tables, Auto Loader, Spark SQL)
* **Python** for transformations
* **Delta Lake** for ACID, versioning, schema evolution
* **Power BI** for data visualization
* **ER/Studio** for dimensional modeling
* **SQL** for data cleaning & modeling

---

## ğŸ” **Key Features**

### âœ” **1. Data Ingestion**

* Ingests CSV files from two different city datasets
* Handles schema differences automatically

### âœ” **2. Data Quality Checks**

Built into DLT pipeline:

* Completeness (null threshold)
* Uniqueness / duplicate detection
* Out-of-range values
* Schema drift identification
* Referential integrity between dimensions/facts

### âœ” **3. Dimensional Modeling**

Designed a **star schema** including:

* `dim_facility`
* `dim_location`
* `dim_violation`
* `fact_inspection`

### âœ” **4. Power BI Dashboard**

Provides insights such as:

* Violations trends
* Risk level distribution
* Inspection frequency
* City-level comparisons
* High-risk facilities

---

## ğŸ“Š **Screenshots (Optional to Add)**

You can upload and embed dashboard/architecture images here.

---

## ğŸ“¦ **How to Run**

### **1. Import the notebook**

Upload `FoodInspectionDLT.ipynb` into Databricks workspace.

### **2. Create a DLT Pipeline**

* Choose **Notebook Pipeline**
* Set target database: `food_inspection_project`
* Set storage location: `/Volumes/.../food_inspection`
* Run pipeline

### **3. Connect Power BI**

* Export Gold-layer tables to Power BI
* Open `Food_Inspection_Dashboard.pbix`

---

## ğŸ“˜ **ER Model**

The `.DM1` file contains the full conceptual + logical schema built using ER/Studio, matching the Gold-layer output.

---

## ğŸ™Œ **Contributors**

* **Ananya Maurya** â€” Data Engineering, ETL, Modeling, Dashboards
* **Nitish Chowdary K** â€” Project Support

---

## â­ **Future Enhancements**

* Add Great Expectations for advanced data quality
* Automate dashboard refresh
* Deploy orchestration via Airflow / ADF
* Add unit tests for pipeline validation

---

# ğŸ‰ **If you want, I can also generate:**

âœ” A professional project description for your **resume**
âœ” A **LinkedIn post** showcasing this project
âœ” An **architecture diagram**
âœ” A **GIF animation** of the pipeline flow

Just tell me **â€œmake project summaryâ€** or **â€œmake LinkedIn postâ€**.
